{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc723743",
   "metadata": {},
   "source": [
    "# üìò **How `wget` and `requests` Obtain HTML: Internal Mechanics and Network Processes**\n",
    "\n",
    "Web scraping begins with a fundamental operation: **retrieving the HTML of a page**. Tools such as **`wget`**, **`curl`**, or Python‚Äôs **`requests`** library perform this action by communicating with a web server through the **HTTP/S protocol**. Although both ultimately achieve the same goal, their internal behavior and flexibility differ significantly.\n",
    "\n",
    "This section explains:\n",
    "\n",
    "* How `wget` retrieves pages\n",
    "* How Python‚Äôs `requests` retrieves pages\n",
    "* When and why each is used in scraping\n",
    "\n",
    "---\n",
    "\n",
    "## üü¶ **1. How `wget` Works Internally**\n",
    "\n",
    "`wget` is a **command-line HTTP client** designed for:\n",
    "\n",
    "* Recursively downloading sites\n",
    "* Mirroring servers\n",
    "* Handling files and binary data\n",
    "\n",
    "### **Key Characteristics**\n",
    "\n",
    "| Feature            | Details                        |\n",
    "| ------------------ | ------------------------------ |\n",
    "| Environment        | Linux CLI tool                 |\n",
    "| Header control     | Limited compared to `requests` |\n",
    "| JavaScript support | None (static pages only)       |\n",
    "| Cookies            | Basic support                  |\n",
    "| Redirects          | Automatic                      |\n",
    "| Speed              | Very fast for simple downloads |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Download HTML with `wget`**\n",
    "\n",
    "```bash\n",
    "wget https://example.com\n",
    "```\n",
    "\n",
    "The process:\n",
    "\n",
    "1. Parse URL\n",
    "2. Resolve DNS\n",
    "3. TCP/TLS handshake\n",
    "4. Send GET request\n",
    "5. Write received HTML to a file named `index.html`\n",
    "\n",
    "---\n",
    "\n",
    "### **Custom Headers (limited)**\n",
    "\n",
    "```bash\n",
    "wget --header=\"User-Agent: Mozilla/5.0\" https://example.com\n",
    "```\n",
    "\n",
    "Headers affect scraping legality, server acceptance, and access capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## üü¶ **2. How Python's `requests` Works Internally (Detailed Focus)**\n",
    "\n",
    "`requests` is built on top of:\n",
    "\n",
    "* **urllib3**\n",
    "* **httplib (http.client)**\n",
    "* **OpenSSL** for HTTPS\n",
    "\n",
    "It abstracts all complexities into a clean API but still performs the full HTTP sequence explained earlier.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Internal Workflow of `requests.get()`**\n",
    "\n",
    "When you write:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "r = requests.get(\"https://example.com\")\n",
    "```\n",
    "\n",
    "Internally:\n",
    "\n",
    "#### **1. URL is parsed**\n",
    "\n",
    "`requests` extracts protocol, host, port, path.\n",
    "\n",
    "#### **2. Session object uses urllib3 to:**\n",
    "\n",
    "* Perform DNS lookup\n",
    "* Open TCP connection\n",
    "* Perform TLS handshake (HTTPS)\n",
    "\n",
    "#### **3. HTTP GET request is built**\n",
    "\n",
    "Headers added automatically:\n",
    "\n",
    "```\n",
    "User-Agent: python-requests/2.x.x\n",
    "Accept-Encoding: gzip, deflate\n",
    "Accept: */*\n",
    "Connection: keep-alive\n",
    "```\n",
    "\n",
    "#### **4. Request is sent over TCP**\n",
    "\n",
    "#### **5. Response is received**\n",
    "\n",
    "* Status code (e.g., `200`)\n",
    "* Headers\n",
    "* Body (HTML or JSON)\n",
    "\n",
    "#### **6. Response body is decoded**\n",
    "\n",
    "* gzip or deflate decompressed\n",
    "* charset decoded\n",
    "* Text made available as `r.text`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úîÔ∏è Basic HTML Download with `requests`\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "html = response.text\n",
    "print(html)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úîÔ∏è Adding Headers (important for scraping)\n",
    "\n",
    "```python\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.8\",\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "```\n",
    "\n",
    "Websites often block default Python user agents.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úîÔ∏è Handling Cookies\n",
    "\n",
    "```python\n",
    "response = requests.get(url)\n",
    "cookies = response.cookies\n",
    "```\n",
    "\n",
    "Cookies may be required for authenticated or persistent scraping.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úîÔ∏è Connection Pooling with Sessions\n",
    "\n",
    "```python\n",
    "s = requests.Session()\n",
    "r = s.get(\"https://example.com\")\n",
    "```\n",
    "\n",
    "Sessions keep:\n",
    "\n",
    "* Cookies\n",
    "* Headers\n",
    "* Connections (keep-alive)\n",
    "\n",
    "This reduces latency for repeated scraping.\n",
    "\n",
    "---\n",
    "\n",
    "## üü¶ **3. When to Use `wget` vs. `requests`**\n",
    "\n",
    "| Situation                         | Use `wget` | Use `requests`     |\n",
    "| --------------------------------- | ---------- | ------------------ |\n",
    "| Simple download of HTML or files  | ‚úîÔ∏è         | ‚úîÔ∏è                 |\n",
    "| Web scraping with logic           | ‚ùå          | ‚úîÔ∏è                 |\n",
    "| Custom headers                    | ‚ö†Ô∏è Limited | ‚úîÔ∏è Advanced        |\n",
    "| Managing cookies                  | Basic      | Full               |\n",
    "| Need to interact programmatically | ‚ùå          | ‚úîÔ∏è                 |\n",
    "| Handling forms, APIs              | ‚ùå          | ‚úîÔ∏è                 |\n",
    "| Large-scale automation            | ‚úîÔ∏è         | ‚úîÔ∏è (with sessions) |\n",
    "\n",
    "**Conclusion:**\n",
    "For data science and scraping, **`requests` is the standard tool**, while `wget` is useful for basic, static retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò **Advanced Guide to `requests`: Configurations, Parameters, and Techniques for Web Scraping**\n",
    "\n",
    "Python‚Äôs `requests` is a high-level HTTP client library built to make network communication *simple*, *readable*, and *powerful*. Although calling `requests.get()` is straightforward, professional scraping requires mastering its advanced capabilities:\n",
    "\n",
    "* Custom headers\n",
    "* Cookies and sessions\n",
    "* Authentication\n",
    "* Query parameters\n",
    "* Timeouts\n",
    "* Error handling\n",
    "* Redirect control\n",
    "* File downloads\n",
    "* Streaming responses\n",
    "* Proxy usage\n",
    "* SSL configuration\n",
    "* Retries\n",
    "\n",
    "Below you will find a **detailed and practical explanation** of each of these features.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 1. **HTTP Methods**\n",
    "\n",
    "Although `GET` is most common in scraping, you should know the others.\n",
    "\n",
    "```python\n",
    "requests.get(url)\n",
    "requests.post(url, data={})\n",
    "requests.put(url, data={})\n",
    "requests.delete(url)\n",
    "```\n",
    "\n",
    "* **GET** ‚Üí retrieve information (HTML, JSON, images)\n",
    "* **POST** ‚Üí submit forms, login, APIs\n",
    "* **PUT / DELETE** ‚Üí less common in scraping, used for RESTful APIs\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 2. **Headers (Critical for Scraping)**\n",
    "\n",
    "Headers define how your client ‚Äúbehaves‚Äù when talking to servers.\n",
    "\n",
    "#### The most important header: `User-Agent`\n",
    "\n",
    "Many websites block default Python agents.\n",
    "\n",
    "```python\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "```\n",
    "\n",
    "#### Other useful headers\n",
    "\n",
    "```python\n",
    "headers = {\n",
    "    \"User-Agent\": \"...\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml\",\n",
    "    \"Referer\": \"https://google.com\",\n",
    "}\n",
    "```\n",
    "\n",
    "**Purpose of headers:**\n",
    "\n",
    "* Avoid blocks\n",
    "* Mimic real browsers\n",
    "* Handle localization\n",
    "* Access hidden/conditional content\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 3. **Query Parameters (GET Params)**\n",
    "\n",
    "You can attach parameters using the `params` argument:\n",
    "\n",
    "```python\n",
    "params = {\"search\": \"python\", \"page\": 2}\n",
    "response = requests.get(url, params=params)\n",
    "```\n",
    "\n",
    "`requests` will turn this into:\n",
    "\n",
    "```\n",
    "GET /?search=python&page=2 HTTP/1.1\n",
    "```\n",
    "\n",
    "Used heavily when interacting with pagination, search filters, or APIs.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 4. **POST Requests and Form Submission**\n",
    "\n",
    "Forms use `POST`, not `GET`.\n",
    "\n",
    "```python\n",
    "payload = {\"username\": \"ariadna\", \"password\": \"1234\"}\n",
    "response = requests.post(url, data=payload)\n",
    "```\n",
    "\n",
    "This is equivalent to submitting an HTML form.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 5. **JSON Requests (Common in APIs)**\n",
    "\n",
    "```python\n",
    "response = requests.post(url, json={\"key\": \"value\"})\n",
    "data = response.json()\n",
    "```\n",
    "\n",
    "`json=` automatically sets:\n",
    "\n",
    "```\n",
    "Content-Type: application/json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 6. **Cookies Handling**\n",
    "\n",
    "Servers often use cookies to manage logins or sessions.\n",
    "\n",
    "#### Get cookies returned by the server:\n",
    "\n",
    "```python\n",
    "response = requests.get(url)\n",
    "print(response.cookies)\n",
    "```\n",
    "\n",
    "#### Send your own cookies:\n",
    "\n",
    "```python\n",
    "cookies = {\"session_id\": \"ABC123\"}\n",
    "response = requests.get(url, cookies=cookies)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 7. **Sessions (Persistent Connections, Cookies, and Headers)**\n",
    "\n",
    "A `Session` object keeps:\n",
    "\n",
    "* Cookies\n",
    "* Connection reuse (Keep-Alive)\n",
    "* Default headers\n",
    "* Authentication\n",
    "\n",
    "```python\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "r1 = session.get(url)\n",
    "r2 = session.get(url2)\n",
    "```\n",
    "\n",
    "**Why sessions matter in scraping:**\n",
    "\n",
    "* Faster (connection pooling)\n",
    "* Needed for login flows\n",
    "* Maintains cookies (maintains login state)\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 8. **Timeouts (Critical for avoiding freezes)**\n",
    "\n",
    "Always set a timeout to avoid infinite wait.\n",
    "\n",
    "```python\n",
    "response = requests.get(url, timeout=5)  \n",
    "```\n",
    "\n",
    "Timeout applies to:\n",
    "\n",
    "* Connection establishment\n",
    "* Server response delay\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 9. **Error and Status Code Handling**\n",
    "\n",
    "#### Basic:\n",
    "\n",
    "```python\n",
    "if response.status_code == 200:\n",
    "    ...\n",
    "```\n",
    "\n",
    "#### More robust:\n",
    "\n",
    "```python\n",
    "response.raise_for_status()  # raises exceptions for 4xx/5xx\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 10. **Disable or Control Redirects**\n",
    "\n",
    "By default, redirects are followed.\n",
    "\n",
    "```python\n",
    "response = requests.get(url, allow_redirects=False)\n",
    "```\n",
    "\n",
    "This is useful when investigating security, logins, or anti-scraping behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 11. **Downloading Files**\n",
    "\n",
    "```python\n",
    "url = \"https://example.com/file.pdf\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"file.pdf\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "```\n",
    "\n",
    "#### Large files: use streaming\n",
    "\n",
    "```python\n",
    "response = requests.get(url, stream=True)\n",
    "for chunk in response.iter_content(chunk_size=1024):\n",
    "    if chunk:\n",
    "        f.write(chunk)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 12. **Streaming Responses (Avoid Memory Overload)**\n",
    "\n",
    "Useful for:\n",
    "\n",
    "* Large pages\n",
    "* Live data\n",
    "* Big files\n",
    "\n",
    "```python\n",
    "response = requests.get(url, stream=True)\n",
    "```\n",
    "\n",
    "You read the content progressively rather than loading everything into RAM.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 13. **Using Proxies**\n",
    "\n",
    "Necessary for:\n",
    "\n",
    "* Geo-targeting\n",
    "* Rotating IPs\n",
    "* Avoiding bans\n",
    "\n",
    "```python\n",
    "proxies = {\n",
    "    \"http\": \"http://123.123.123.123:8080\",\n",
    "    \"https\": \"https://123.123.123.123:8080\",\n",
    "}\n",
    "response = requests.get(url, proxies=proxies)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 14. **Authentication**\n",
    "\n",
    "#### Basic Auth:\n",
    "\n",
    "```python\n",
    "requests.get(url, auth=('user', 'pass'))\n",
    "```\n",
    "\n",
    "#### Token-based:\n",
    "\n",
    "```python\n",
    "headers = {\"Authorization\": \"Bearer <TOKEN>\"}\n",
    "requests.get(url, headers=headers)\n",
    "```\n",
    "\n",
    "Common in APIs and private dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 15. **SSL Certificate Verification**\n",
    "\n",
    "Enabled by default (recommended).\n",
    "\n",
    "Disable only when testing:\n",
    "\n",
    "```python\n",
    "requests.get(url, verify=False)\n",
    "```\n",
    "\n",
    "Fear: MITM attacks\n",
    "Use only in controlled networks.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 16. **Retry Logic (Very Important for Stability)**\n",
    "\n",
    "`requests` does NOT retry automatically.\n",
    "You must configure retries yourself:\n",
    "\n",
    "```python\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "response = session.get(url)\n",
    "```\n",
    "\n",
    "Retries prevent your scraper from breaking due to:\n",
    "\n",
    "* Temporary server overload\n",
    "* Network hiccups\n",
    "* Proxy failures\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 17. **Handling Gzip/Deflate Compression**\n",
    "\n",
    "`requests` automatically sends:\n",
    "\n",
    "```\n",
    "Accept-Encoding: gzip, deflate\n",
    "```\n",
    "\n",
    "And decompresses in:\n",
    "\n",
    "```python\n",
    "response.text\n",
    "```\n",
    "\n",
    "Meaning you always receive readable HTML.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 18. **Encoding Control**\n",
    "\n",
    "Sometimes pages send wrong encodings.\n",
    "\n",
    "```python\n",
    "response.encoding = \"utf-8\"\n",
    "html = response.text\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ 19. **Handling Redirections, History, and Chain Requests**\n",
    "\n",
    "```python\n",
    "response = requests.get(url)\n",
    "print(response.history)\n",
    "```\n",
    "\n",
    "Useful for login sequences or anti-bot redirection loops.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Summary: Most Important Features for Scraping\n",
    "\n",
    "| Feature        | Why It Matters                  |\n",
    "| -------------- | ------------------------------- |\n",
    "| Headers        | Avoid blocks; mimic browsers    |\n",
    "| Sessions       | Keep cookies, reuse connections |\n",
    "| Cookies        | Maintain login state            |\n",
    "| Timeouts       | Avoid freezes                   |\n",
    "| Retries        | Handle unstable websites        |\n",
    "| Proxies        | Avoid bans, geolocation control |\n",
    "| Streams        | Handle large files              |\n",
    "| Params & POST  | Interact with web apps          |\n",
    "| SSL, redirects | Control connection behavior     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f85971",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51832811",
   "metadata": {},
   "source": [
    "# üìò **Understanding HTML for Web Scraping (Static Pages)**\n",
    "\n",
    "Static HTML refers to webpages where the content is delivered **directly by the server**, without requiring JavaScript to modify or generate it.\n",
    "When a scraper downloads such a page, the HTML it receives already contains the text, tags, and data that appear in the browser.\n",
    "\n",
    "To extract data correctly, you must understand:\n",
    "\n",
    "* How HTML is structured\n",
    "* How tags and attributes work\n",
    "* How CSS classes and IDs allow identification of elements\n",
    "* How nesting and DOM hierarchy affect extraction\n",
    "* How lists, tables, links, and forms are represented\n",
    "* How to distinguish meaningful from decorative markup\n",
    "\n",
    "Let us examine each of these elements in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 1. **HTML as a Tree (Document Object Model)**\n",
    "\n",
    "HTML is not just text; it is a **hierarchical tree** structure.\n",
    "\n",
    "Example (simplified):\n",
    "\n",
    "```html\n",
    "<html>\n",
    "  <body>\n",
    "    <h1>Title</h1>\n",
    "    <p class=\"description\">This is a description.</p>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Hierarchy:\n",
    "\n",
    "* `<html>` (root)\n",
    "\n",
    "  * `<body>`\n",
    "\n",
    "    * `<h1>`\n",
    "    * `<p>`\n",
    "\n",
    "Why this matters for scraping:\n",
    "\n",
    "* BeautifulSoup and lxml interpret HTML as a tree\n",
    "* You navigate parent ‚Üí child ‚Üí sibling\n",
    "* Precise extraction depends on understanding this structure\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 2. **Tags (Elements): The Fundamental Units**\n",
    "\n",
    "Tags define what each part of the document *means*.\n",
    "\n",
    "Some of the most relevant tags for scrapers are:\n",
    "\n",
    "| Tag                    | Purpose             | Relevance for Scraping                                    |\n",
    "| ---------------------- | ------------------- | --------------------------------------------------------- |\n",
    "| `<div>`                | Generic container   | Frequently used; usually requires class or ID to identify |\n",
    "| `<span>`               | Inline container    | Often used for labels, metadata                           |\n",
    "| `<a>`                  | Link                | Critical for pagination, crawling                         |\n",
    "| `<img>`                | Images              | Scrapers need `src` attribute                             |\n",
    "| `<ul>`, `<ol>`, `<li>` | Lists               | Used for menus, product lists                             |\n",
    "| `<table>`              | Structured data     | Easy to scrape into datasets                              |\n",
    "| `<form>`               | Login, search       | Important for automating interactions                     |\n",
    "| `<input>`              | Form fields         | Required for POST requests                                |\n",
    "| `<script>`             | JavaScript code     | Can reveal API endpoints                                  |\n",
    "| `<meta>`               | Encodings, metadata | Useful for page information                               |\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 3. **Attributes (critical for identifying elements)**\n",
    "\n",
    "Attributes are key/value pairs inside tags.\n",
    "\n",
    "Example:\n",
    "\n",
    "```html\n",
    "<p class=\"summary\" id=\"product-description\" data-id=\"123\">\n",
    "    Great product!\n",
    "</p>\n",
    "```\n",
    "\n",
    "Important attributes for scraping:\n",
    "\n",
    "| Attribute | Description            | Usage in Scraping                    |\n",
    "| --------- | ---------------------- | ------------------------------------ |\n",
    "| `id`      | Unique identifier      | Best for selecting specific elements |\n",
    "| `class`   | Category/group label   | Most common selector in modern HTML  |\n",
    "| `href`    | Link URL               | Used to crawl pages                  |\n",
    "| `src`     | Image or script source | Downloads, media scraping            |\n",
    "| `data-*`  | Custom metadata        | Hidden but extremely useful          |\n",
    "| `name`    | Input field name       | Required for form submission         |\n",
    "| `content` | Meta tag content       | Useful for metadata extraction       |\n",
    "\n",
    "### Why attributes matter:\n",
    "\n",
    "Websites rarely use unique structure; they use **classes**, **IDs**, and **data attributes** so scrapers can reliably locate elements.\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 4. **Classes and IDs: The Backbone of Scraper Selectors**\n",
    "\n",
    "### ‚úîÔ∏è IDs should be unique\n",
    "\n",
    "Ideal selector:\n",
    "\n",
    "```html\n",
    "<div id=\"main-title\">Product A</div>\n",
    "```\n",
    "\n",
    "In BeautifulSoup:\n",
    "\n",
    "```python\n",
    "soup.find(id=\"main-title\")\n",
    "```\n",
    "\n",
    "### ‚úîÔ∏è Classes identify groups of elements\n",
    "\n",
    "Example:\n",
    "\n",
    "```html\n",
    "<div class=\"product\">...</div>\n",
    "<div class=\"product\">...</div>\n",
    "<div class=\"product\">...</div>\n",
    "```\n",
    "\n",
    "In BeautifulSoup:\n",
    "\n",
    "```python\n",
    "soup.find_all(class_=\"product\")\n",
    "```\n",
    "\n",
    "**Classes and IDs are the most important parts of HTML for scraping**, because they allow you to locate data precisely and reliably.\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 5. **Nesting and Hierarchy (Critical for Navigation)**\n",
    "\n",
    "HTML elements are usually nested:\n",
    "\n",
    "```html\n",
    "<div class=\"product\">\n",
    "    <h2 class=\"name\">Laptop X</h2>\n",
    "    <span class=\"price\">$999</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "To extract the name and price:\n",
    "\n",
    "1. Identify the `product` container\n",
    "2. Extract children elements (`h2`, `span`)\n",
    "\n",
    "BeautifulSoup example:\n",
    "\n",
    "```python\n",
    "product = soup.find(\"div\", class_=\"product\")\n",
    "name = product.find(\"h2\").text\n",
    "price = product.find(\"span\", class_=\"price\").text\n",
    "```\n",
    "\n",
    "Understanding nesting allows:\n",
    "\n",
    "* Filtering complex data\n",
    "* Extracting elements that depend on context\n",
    "* Avoiding errors when multiple sections share the same classes\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 6. **Tables (Perfect for Structured Data)**\n",
    "\n",
    "Tables are a goldmine for data scientists:\n",
    "\n",
    "```html\n",
    "<table id=\"stats-table\">\n",
    "  <tr>\n",
    "    <th>Name</th><th>Value</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Height</td><td>180</td>\n",
    "  </tr>\n",
    "</table>\n",
    "```\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Find table\n",
    "2. Extract header rows\n",
    "3. Extract data rows\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 7. **Lists (Common in Product Listings, Menus, Comments)**\n",
    "\n",
    "HTML lists:\n",
    "\n",
    "```html\n",
    "<ul class=\"items\">\n",
    "  <li>Item A</li>\n",
    "  <li>Item B</li>\n",
    "</ul>\n",
    "```\n",
    "\n",
    "Useful for iterating through repeated structures.\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 8. **Links: Navigating the Website (Crawling)**\n",
    "\n",
    "`<a>` tags define navigation:\n",
    "\n",
    "```html\n",
    "<a href=\"/product/123\">View product</a>\n",
    "```\n",
    "\n",
    "Scraper logic:\n",
    "\n",
    "* Extract `href`\n",
    "* Convert to full URL if needed\n",
    "* Follow link\n",
    "\n",
    "Handling relative paths is essential:\n",
    "\n",
    "`requests.get(base_url + href)`\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 9. **Forms: Behind Logins, Searches, Filters**\n",
    "\n",
    "Example:\n",
    "\n",
    "```html\n",
    "<form action=\"/search\" method=\"post\">\n",
    "    <input type=\"text\" name=\"query\">\n",
    "    <input type=\"submit\">\n",
    "</form>\n",
    "```\n",
    "\n",
    "To simulate a form submission:\n",
    "\n",
    "1. Extract `action`\n",
    "2. Extract `method`\n",
    "3. Extract all inputs with `name`\n",
    "\n",
    "Example with `requests`:\n",
    "\n",
    "```python\n",
    "payload = {\"query\": \"python\"}\n",
    "requests.post(\"https://example.com/search\", data=payload)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 10. **Meta Tags (Encoding, Description, Keywords)**\n",
    "\n",
    "Example:\n",
    "\n",
    "```html\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"description\" content=\"Product info\">\n",
    "```\n",
    "\n",
    "Useful for:\n",
    "\n",
    "* Setting `response.encoding`\n",
    "* Understanding content language\n",
    "* Extracting SEO metadata\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 11. **Comments and Hidden Elements**\n",
    "\n",
    "HTML sometimes hides data:\n",
    "\n",
    "```html\n",
    "<div style=\"display:none\">Secret</div>\n",
    "```\n",
    "\n",
    "Or:\n",
    "\n",
    "```html\n",
    "<!-- Price: $499 -->\n",
    "```\n",
    "\n",
    "JavaScript frameworks often put JSON inside `<script>` tags:\n",
    "\n",
    "```html\n",
    "<script id=\"data\">\n",
    "  {\"price\": 100, \"name\": \"Laptop\"}\n",
    "</script>\n",
    "```\n",
    "\n",
    "Scrapers frequently extract this to avoid parsing complex DOM trees.\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 12. **Patterns to Recognize in Static HTML Scraping**\n",
    "\n",
    "‚úîÔ∏è **Repeated structures**\n",
    "Product listings\n",
    "Forum threads\n",
    "Tables\n",
    "\n",
    "‚úîÔ∏è **Key-value structures**\n",
    "Profile pages\n",
    "Specs pages\n",
    "Metadata blocks\n",
    "\n",
    "‚úîÔ∏è **Hidden JSON inside script tags**\n",
    "Used by modern websites as data storage\n",
    "Easier to parse than HTML\n",
    "\n",
    "‚úîÔ∏è **Breadcrumbs (navigation hierarchy)**\n",
    "Useful for categorizing items\n",
    "\n",
    "‚úîÔ∏è **Relative vs. absolute URLs**\n",
    "Important for crawling the entire site\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ 13. **Non-useful Parts of HTML (Usually Ignored)**\n",
    "\n",
    "* CSS styling\n",
    "* Layout-related divs (unless they contain structured content)\n",
    "* JavaScript code (unless it contains hidden data)\n",
    "* Decorative icons\n",
    "* Advertisements\n",
    "\n",
    "A scraper should focus only on **semantic** HTML elements.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úîÔ∏è Summary: What Matters Most in Static HTML Scraping\n",
    "\n",
    "| Element          | Critical Role                        |\n",
    "| ---------------- | ------------------------------------ |\n",
    "| **Tags**         | Define structure of content          |\n",
    "| **Classes/IDs**  | Provide stable selectors             |\n",
    "| **Attributes**   | Contain actual data or links         |\n",
    "| **Hierarchy**    | Determines extraction strategy       |\n",
    "| **Lists/Tables** | Perfect sources of structured data   |\n",
    "| **Links**        | Enable crawling and exploration      |\n",
    "| **Forms**        | Enable interactions (search, login)  |\n",
    "| **Meta tags**    | Control encoding and metadata        |\n",
    "| **Hidden data**  | Often contains essential information |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3703ff8",
   "metadata": {},
   "source": [
    "# üìò **C√≥mo BeautifulSoup Navega el HTML en Web Scraping**\n",
    "\n",
    "## 1. üß© ¬øQu√© es BeautifulSoup y qu√© hace internamente?\n",
    "\n",
    "BeautifulSoup es una librer√≠a de Python dise√±ada para:\n",
    "\n",
    "1. **Parsear HTML o XML**.\n",
    "2. Transformarlo en una estructura tipo √°rbol (*parse tree*).\n",
    "3. Permitir recorrer y buscar elementos usando:\n",
    "\n",
    "   * Nombres de etiquetas\n",
    "   * Atributos\n",
    "   * Texto\n",
    "   * Selectores CSS\n",
    "   * Estructura del DOM\n",
    "\n",
    "### ¬øQu√© hace internamente?\n",
    "\n",
    "Cuando le pasas el HTML, BeautifulSoup ejecuta:\n",
    "\n",
    "1. **Parsing** con un parser interno (`html.parser`) u otros m√°s robustos (`lxml`, `html5lib`).\n",
    "2. Construye una estructura tipo **DOM tree**.\n",
    "3. Cada nodo es un objeto Python del tipo:\n",
    "\n",
    "   * `Tag`\n",
    "   * `NavigableString`\n",
    "   * `Comment`\n",
    "   * `Doctype`\n",
    "\n",
    "### Ejemplo m√≠nimo:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <h1 class=\"title\">Hola</h1>\n",
    "    <p id=\"intro\">Bienvenida al scraping</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(soup.prettify())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üå≥ **El √Årbol DOM en BeautifulSoup**\n",
    "\n",
    "BeautifulSoup convierte el HTML en nodos que puedes recorrer como si fueran un √°rbol.\n",
    "\n",
    "### Tipos de nodos:\n",
    "\n",
    "| Tipo              | Descripci√≥n                                 |\n",
    "| ----------------- | ------------------------------------------- |\n",
    "| `Tag`             | Representa una etiqueta (`<div>, <p>, <a>`) |\n",
    "| `NavigableString` | Representa texto dentro de una etiqueta     |\n",
    "| `Comment`         | Representa un comentario `<!-- ... -->`     |\n",
    "| `BeautifulSoup`   | Objeto ra√≠z del documento                   |\n",
    "\n",
    "### Ejemplo visual:\n",
    "\n",
    "HTML:\n",
    "\n",
    "```html\n",
    "<div>\n",
    "    <p>Hola <b>mundo</b></p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "√Årbol conceptual:\n",
    "\n",
    "```\n",
    "div\n",
    " ‚îî‚îÄ‚îÄ p\n",
    "      ‚îú‚îÄ‚îÄ \"Hola \"\n",
    "      ‚îî‚îÄ‚îÄ b\n",
    "           ‚îî‚îÄ‚îÄ \"mundo\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. üîç B√∫squeda en el DOM: `find()` y `find_all()`\n",
    "\n",
    "### ‚úî `find()` ‚Üí primera coincidencia\n",
    "\n",
    "### ‚úî `find_all()` ‚Üí todas las coincidencias\n",
    "\n",
    "### Buscar por etiqueta:\n",
    "\n",
    "```python\n",
    "soup.find(\"p\")\n",
    "soup.find_all(\"p\")\n",
    "```\n",
    "\n",
    "### Buscar por clase:\n",
    "\n",
    "```python\n",
    "soup.find(\"h1\", class_=\"title\")\n",
    "soup.find_all(\"div\", class_=\"item\")\n",
    "```\n",
    "\n",
    "### Buscar por id:\n",
    "\n",
    "```python\n",
    "soup.find(id=\"intro\")\n",
    "```\n",
    "\n",
    "### Buscar por varios atributos:\n",
    "\n",
    "```python\n",
    "soup.find(\"a\", {\"href\": True, \"class\": \"link\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. üéØ B√∫squeda avanzada con funciones personalizadas\n",
    "\n",
    "Puedes usar una funci√≥n para filtrar elementos:\n",
    "\n",
    "```python\n",
    "def is_download_link(tag):\n",
    "    return tag.name == \"a\" and tag.get(\"href\", \"\").endswith(\".zip\")\n",
    "\n",
    "links = soup.find_all(is_download_link)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. üé® Uso de **selectores CSS** (muy potente)\n",
    "\n",
    "BeautifulSoup permite buscar como si usaras CSS:\n",
    "\n",
    "| Selector            | Significado                       |\n",
    "| ------------------- | --------------------------------- |\n",
    "| `\"div p\"`           | `<p>` dentro de `<div>`           |\n",
    "| `\".clase\"`          | elementos con esa clase           |\n",
    "| `\"#id\"`             | elemento con ese id               |\n",
    "| `\"div > p\"`         | `<p>` hijo directo de `<div>`     |\n",
    "| `\"a[href]\"`         | `<a>` con atributo `href`         |\n",
    "| `\"a[href$='.pdf']\"` | `<a>` cuyo href termina en \".pdf\" |\n",
    "\n",
    "### Ejemplo:\n",
    "\n",
    "```python\n",
    "soup.select(\"div.item > a.link\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. üìÇ Acceso a contenido: texto, atributos y estructura\n",
    "\n",
    "### Obtener texto:\n",
    "\n",
    "```python\n",
    "tag = soup.find(\"p\")\n",
    "tag.text          # todo el texto interno\n",
    "tag.get_text()    # m√°s robusto\n",
    "```\n",
    "\n",
    "### Obtener un atributo:\n",
    "\n",
    "```python\n",
    "a = soup.find(\"a\")\n",
    "a[\"href\"]\n",
    "```\n",
    "\n",
    "### Obtener m√∫ltiples atributos:\n",
    "\n",
    "```python\n",
    "a.attrs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. üß≠ Navegaci√≥n del DOM: recorrer padres, hijos y hermanos\n",
    "\n",
    "BeautifulSoup permite recorrer la estructura como si fuera un √°rbol.\n",
    "\n",
    "### ‚ñº 7.1 Hijos\n",
    "\n",
    "```python\n",
    "tag.contents   # lista de hijos\n",
    "tag.children   # generador\n",
    "```\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "for child in tag.children:\n",
    "    print(child)\n",
    "```\n",
    "\n",
    "### ‚ñº 7.2 Padre\n",
    "\n",
    "```python\n",
    "tag.parent\n",
    "```\n",
    "\n",
    "### ‚ñº 7.3 Ancestros\n",
    "\n",
    "```python\n",
    "for ancestor in tag.parents:\n",
    "    print(ancestor.name)\n",
    "```\n",
    "\n",
    "### ‚ñº 7.4 Hermanos\n",
    "\n",
    "#### Siguiente hermano:\n",
    "\n",
    "```python\n",
    "tag.next_sibling\n",
    "```\n",
    "\n",
    "#### Hermano anterior:\n",
    "\n",
    "```python\n",
    "tag.previous_sibling\n",
    "```\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "h1 = soup.find(\"h1\")\n",
    "h1.next_sibling\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. üßº Limpieza y filtrado del HTML\n",
    "\n",
    "### Eliminar etiquetas:\n",
    "\n",
    "```python\n",
    "tag.decompose()\n",
    "```\n",
    "\n",
    "### Limpiar texto:\n",
    "\n",
    "```python\n",
    "tag.get_text(strip=True)\n",
    "```\n",
    "\n",
    "### Reemplazar elementos:\n",
    "\n",
    "```python\n",
    "tag.string = \"Nuevo texto\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. üìù Ejemplo completo de navegaci√≥n\n",
    "\n",
    "Supongamos este HTML:\n",
    "\n",
    "```html\n",
    "<div class=\"product\">\n",
    "  <h2 class=\"name\">Teclado Mec√°nico</h2>\n",
    "  <span class=\"price\">$25</span>\n",
    "  <a href=\"/compra\" class=\"buy\">Comprar</a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "### Extraer datos:\n",
    "\n",
    "```python\n",
    "product = soup.find(\"div\", class_=\"product\")\n",
    "\n",
    "nombre = product.find(\"h2\", class_=\"name\").text\n",
    "precio = product.find(\"span\", class_=\"price\").text\n",
    "link = product.find(\"a\", class_=\"buy\")[\"href\"]\n",
    "\n",
    "print(nombre, precio, link)\n",
    "```\n",
    "\n",
    "Salida:\n",
    "\n",
    "```\n",
    "Teclado Mec√°nico $25 /compra\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
