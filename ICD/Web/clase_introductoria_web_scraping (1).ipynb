{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9300854",
   "metadata": {},
   "source": [
    "## **Clase 1: Fundamentos y Panorama General del Web Scraping** \n",
    "\n",
    "### **1. Introducción:**\n",
    "\n",
    "\"¿Alguna vez te has preguntado cómo compañías como Amazon o Airbnb obtienen millones de datos de productos o precios en tiempo real? La respuesta es: Web Scraping.\"\n",
    "\n",
    "\"En un mundo donde los datos son el nuevo petróleo, el Web Scraping es la herramienta que nos permite extraerlos de la web.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e9ce5",
   "metadata": {},
   "source": [
    "### **2. Definición de \"Scraping\":**\n",
    "\n",
    "El scraping se define como el acto de extraer datos de cualquier fuente.\n",
    "\n",
    "**Definición General**: La copia manual de datos desde un documento PDF a una hoja de cálculo es una forma de scraping no automatizado.\n",
    "\n",
    "**Definición Específica (Web Scraping)**: Se refiere al **proceso automatizado de extraer datos específicamente de sitios web**. Se construye un script (un \"bot\") que navega por un sitio web, interpreta su estructura y extrae únicamente los elementos de interés.\n",
    "\n",
    "El web scraping se encuentra en la intersección de **recolección de datos, tecnologías web y automatización**. Es una técnica para transformar el **contenido visible de internet** — que a menudo está destinado para humanos — en **conjuntos de datos legibles por máquinas** adecuados para análisis computacional.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca196d4",
   "metadata": {},
   "source": [
    "### **3. Justificación y Aplicaciones:**\n",
    "\n",
    "La necesidad del scraping radica en que los conjuntos de datos más relevantes para un problema específico a menudo no existen de forma predeterminada. Es necesario generarlos. El web scraping funciona como el mecanismo para acortar la brecha entre las preguntas y los datos requeridos para responderlas.\n",
    "\n",
    "Particularmente, el web scraping es una competencia fundamental en el conjunto de herramientas del científico de datos porque conecta el proceso de ciencia de datos con el mundo real, donde los datos sin procesar rara vez están ordenados.\n",
    "\n",
    "| Dimension                  | Importance                                                                                                                            |\n",
    "| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Accesibilidad de Datos** | Muchas fuentes de datos valiosas (tendencias de mercado, comportamiento social, corpus de texto) solo son accesibles a través de la web. El scraping cierra esa brecha. |\n",
    "| **Creación de Datos Personalizados** | Permite a investigadores y analistas generar sus propios conjuntos de datos adaptados a hipótesis o proyectos específicos.                           |\n",
    "| **Automatización y Pipelines** | Se integra perfectamente con los procesos ETL (Extract–Transform–Load) para la recolección continua de datos.                                     |\n",
    "| **Integración de Habilidades** | Involucra programación, comprensión de redes (HTTP) y análisis de datos — fortaleciendo habilidades interdisciplinarias.                        |\n",
    "| **Impacto en el Mundo Real** | Habilita proyectos como monitoreo de precios, análisis de desinformación y seguimiento del sentimiento público.                                       |\n",
    "\n",
    "\n",
    "**Ejemplo 1**: Monitoreo de Precios\n",
    "\n",
    "- **Cuestionamiento:** ¿Cuál es el precio promedio de un apartamento en La Habana tomando los apartmentos publicados en Revolico?\n",
    "\n",
    "- **Método (Scraping):** Se implementa un script que visita Revolico diariamente para recoilar la infomación de los precios.\n",
    "\n",
    "**Ejemplo 2:** Análisis de Sentimiento\n",
    "\n",
    "- **Cuestionamiento:** ¿Cuál es la percepción pública predominante sobre un nuevo producto tecnológico?\n",
    "\n",
    "- **Método (Scraping):** Se extraen las últimas 5,000 reseñas de usuarios de un portal de e-commerce.\n",
    "\n",
    "**Otros Ejemplos**:\n",
    "- Reunir ofertas de trabajo para un análisis del mercado.\n",
    "\n",
    "- Extraer publicaciones académicas para investigación.\n",
    "\n",
    "- Rastrear noticias en trends de redes sociales.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8787162",
   "metadata": {},
   "source": [
    "### **4. Arquitecturas Web Estáticas vs Dinámicas:**\n",
    "\n",
    "#### **Página Estática** (Renderizado en Servidor)\n",
    "\n",
    "- **Cómo funciona:** El servidor web envía un documento HTML completo y pre-renderizado. Todo el contenido textual y estructural está presente en la respuesta inicial.\n",
    "\n",
    "- **Analogía:** Se asemeja a un documento impreso; el contenido es fijo y completo desde el momento de la enterga.\n",
    "\n",
    "- **Herramienta Principal:** requests + BeautifulSoup .\n",
    "\n",
    "\n",
    "#### **Página Dinámica** (Renderizado en Cliente)\n",
    "\n",
    "- **Cómo funciona:** El servidor envía un documento HTML mínimo (un \"esqueleto\"). Posteriormente, el navegador del cliente ejecuta código JavaScript que solicita los datos de forma asíncrona (a menudo a una API) y modifica el DOM (Document Object Model) para renderizar el contenido final.\n",
    "\n",
    "- **Analogía:** El contenido se \"construye\" o \"ensambla\" en el lado del cliente después de la carga inicial.\n",
    "\n",
    "- **El Problema:** Una petición HTTP simple (vía requests) solo recuperará el HTML inicial, carente de los datos de interés.\n",
    "\n",
    "- **Herramienta Principal:** Selenium .\n",
    "\n",
    "**Criterio de Diferenciación:**\n",
    "Una técnica consiste en comparar el \"código fuente\" de la página (lo que recibe requests) con el contenido inspeccionado por las \"Herramientas de Desarrollador\" del navegador (el DOM final, visto por Selenium). Discrepancias significativas sugieren contenido dinámico.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142cdf1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### **5. El Protocolo de Comunicación Web (HTTP):**\n",
    "\n",
    "Para solicitar estos recursos (estáticos o dinámicos), es necesario utilizar el protocolo estándar de la web: HTTP (Hypertext Transfer Protocol).\n",
    "\n",
    "**La interacción se basa en un modelo Cliente-Servidor:**\n",
    "\n",
    "**El Cliente:** Su navegador o, en este caso, su script de Python.\n",
    "\n",
    "**El Servidor:** El sistema donde se aloja el recurso web solicitado.\n",
    "\n",
    "#### **A. Las Peticiones (Métodos HTTP)**\n",
    "\n",
    "- **GET (El más común):** Solicita una representación de un recurso específico (ej. \"TRÁEME la página de inicio\"). Es una operación de solo lectura.\n",
    "\n",
    "- **POST:** Envía datos a un servidor para crear o actualizar un recurso (ej. \"TOMA estos datos de formulario de login\").\n",
    "\n",
    "#### **B. Las Respuestas y sus Códigos de Estado**\n",
    "\n",
    "Un script de scraping robusto debe siempre verificar el código de estado HTTP de la respuesta antes de intentar procesarla.\n",
    "\n",
    "- **200 OK (Éxito):** La solicitud ha tenido éxito. Es el código esperado para una extracción exitosa.\n",
    "\n",
    "- **404 Not Found (Error del Cliente):** El servidor no pudo encontrar el recurso solicitado.\n",
    "\n",
    "- **403 Forbidden (Error del Cliente):** El cliente no posee los permisos necesarios para acceder al recurso. El servidor ha identificado y denegado la solicitud.\n",
    "\n",
    "- **429 Too Many Requests (Error del Cliente):** El usuario ha enviado demasiadas solicitudes en un período de tiempo determinado (Rate Limiting). El script debe pausar su ejecución.\n",
    "\n",
    "- **500 Internal Server Error (Error del Servidor):** El servidor encontró una condición inesperada que le impidió completar la solicitud.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10322351",
   "metadata": {},
   "source": [
    "### **6. Herramientas de Extracción:**\n",
    "\n",
    "##### **Habiendo comprendido el problema (Estático vs. Dinámico) y el protocolo (HTTP), las siguientes herramientas pueden ser evaluadas:**\n",
    "\n",
    "**wget / requests (El Cliente HTTP)**\n",
    "\n",
    "- **Función:** Son clientes HTTP que ejecutan peticiones (ej. GET). Su única función es solicitar un recurso y recibir la respuesta (generalmente el HTML).\n",
    "\n",
    "- **Caso de uso principal:** Páginas Estáticas.\n",
    "\n",
    "**Beautiful Soup (El Parser)**\n",
    "\n",
    "- **Función:** Una vez que requests obtiene el HTML, BeautifulSoup lo parsea (interpreta) y crea un árbol de objetos, permitiendo la navegación y extracción de elementos mediante selectores.\n",
    "\n",
    "- **Caso de uso principal:** Leer y navegar cualquier documento HTML/XML.\n",
    "\n",
    "**Selenium (El Controlador de Navegador)**\n",
    "\n",
    "- **Función:** Automatiza y controla un navegador web real. Al hacerlo, ejecuta el JavaScript de la página, permitiendo el acceso al DOM final renderizado.\n",
    "\n",
    "- **Caso de uso principal:** Páginas Dinámicas (renderizadas en cliente: cargan más resultados al scrolear o requieren clicks para relevelar la información).\n",
    "\n",
    "---\n",
    "### **Ejemplo de scraping estático**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd0ca20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E217881E80>, 'Connection to books.toscrape.com timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectTimeoutError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\connection.py:207\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[32m    208\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    209\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.host\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.timeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    210\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectTimeoutError\u001b[39m: (<urllib3.connection.HTTPSConnection object at 0x000001E217881E80>, 'Connection to books.toscrape.com timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E217881E80>, 'Connection to books.toscrape.com timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectTimeout\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttps://books.toscrape.com/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# realizar la petición GET \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# verificar que la petición fue exitosa\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\requests\\adapters.py:665\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ConnectTimeoutError):\n\u001b[32m    663\u001b[39m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, NewConnectionError):\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request=request)\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ResponseError):\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request=request)\n",
      "\u001b[31mConnectTimeout\u001b[39m: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E217881E80>, 'Connection to books.toscrape.com timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "# Se requiere el uso de VPN \n",
    "\n",
    "#Importar librerías\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# realizar la petición GET \n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "# verificar que la petición fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(\"la petición fue exitosa\")\n",
    "else:\n",
    "    print(f\"la petición falló, error{response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "# Extraer el head \n",
    "head =soup.find(\"title\")\n",
    "print(head.get_text(strip=True))\n",
    "\n",
    "# Extraer el título principal\n",
    "titulo  = soup.find(\"div\", class_=\"col-sm-8 h1\")\n",
    "print(titulo.get_text(strip=True))\n",
    "\n",
    "# Buscar todos los productos\n",
    "productos=soup.select(\"article.product_pod\")\n",
    "\n",
    "lista_productos = []\n",
    "\n",
    "for producto in productos:\n",
    "    nombre = producto.find(\"h3\").find(\"a\")[\"title\"]\n",
    "    precio = producto.find(\"p\", class_=\"price_color\").get_text()\n",
    "    numero_estrellas =producto.find(\"p\",class_=\"star-rating\")\n",
    "    disponibilidad = producto.find(\"p\", class_=\"instock availability\").get_text(strip=True)\n",
    "    \n",
    "    producto_info = {\n",
    "        \"nombre\": nombre,\n",
    "        \"precio\": precio,\n",
    "        \"disponibilidad\": disponibilidad\n",
    "    }\n",
    "    \n",
    "    lista_productos.append(producto_info)\n",
    "\n",
    "print(f\"Se encontraron {len(lista_productos)} productos\")\n",
    "\n",
    "# Guardar los datos en un archivo CSV\n",
    "with open('productos.csv', 'w', newline='', encoding='utf-8') as archivo_csv:\n",
    "    campos = ['nombre', 'precio', 'disponibilidad']\n",
    "    \n",
    "    escritor = csv.DictWriter(archivo_csv, fieldnames=campos)\n",
    "    \n",
    "    escritor.writeheader()\n",
    "    escritor.writerows(lista_productos)\n",
    "\n",
    "print(\"Datos guardados exitosamente en 'productos.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fcd045",
   "metadata": {},
   "source": [
    "\n",
    "### **7. Comparativa de Métodos: Scraping vs. Consumo de API**\n",
    "\n",
    "#### **Scraping :**\n",
    "\n",
    "- **Analogía:** Se asemeja a interpretar la vitrina de una tienda (el HTML) para deducir el inventario.\n",
    "\n",
    "- **Problema:** Alta fragilidad. Modificaciones en la estructura del DOM (rediseños web) invalidan el extractor y requieren mantenimiento.\n",
    "\n",
    "#### **Consumir una API :**\n",
    "\n",
    "- **Analogía:** Solicitar formalmente al gerente de la tienda (la API) el inventario (los datos) en un formato estructurado (ej. JSON).\n",
    "\n",
    "- **Definición:** Una API (Application Programming Interface) provee un punto de acceso diseñado para la comunicación máquina-a-máquina, entregando datos limpios.\n",
    "\n",
    "**Principio Rector**: Se debe priorizar el uso de una API oficial siempre que esté disponible. El scraping es un método secundario para cuando no existen APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14cf32",
   "metadata": {},
   "source": [
    "### **8. Consideraciones Éticas y Legales:**\n",
    "\n",
    "La práctica del web scraping debe adherirse a principios éticos estrictos. El objetivo es la recolección de datos, no la disrupción del servicio.\n",
    "\n",
    "**Consultar robots.txt:**\n",
    "\n",
    "Todo dominio debe tener un archivo tusitio.com/robots.txt que especifica las directrices para bots. Es mandatorio respetarlo.\n",
    "\n",
    "**Implementar Limitación de Tasa (Rate Limiting):**\n",
    "\n",
    "Evitar la saturación del servidor. No se deben realizar miles de peticiones simultáneas.\n",
    "\n",
    "Implementar pausas deliberadas **(ej. time.sleep(2))** entre peticiones es una práctica estándar.\n",
    "\n",
    "**Revisar los Términos de Servicio (ToS):**\n",
    "\n",
    "Muchos sitios prohíben explícitamente el scraping en sus Términos de Servicio. Su violación puede tener implicaciones legales.\n",
    "\n",
    "**Identificación (User-Agent):**\n",
    "\n",
    "Es una buena práctica identificar a su bot mediante un User-Agent descriptivo, en lugar de enmascararlo como un navegador estándar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f7152",
   "metadata": {},
   "source": [
    "## Tarea\n",
    "Hacer un programa que scrapee cubadebate y obtenga los títulos de 30 noticias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
